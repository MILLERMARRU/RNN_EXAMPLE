# PredicciÃ³n de Siguiente Palabra con RNN - Proyecto de Saludos

## ğŸ“‹ Objetivo
Entrenar una Red Neuronal Recurrente (RNN) para predecir la siguiente palabra en frases de saludo en espaÃ±ol.

---

## ğŸ“ Estructura del Proyecto

```
ejemploRNN/
â”œâ”€â”€ saludos_dataset.txt              # Dataset con 93 frases de saludo
â”œâ”€â”€ prediccion_saludos_RNN.py        # Script principal de entrenamiento
â”œâ”€â”€ inferencia_interactiva.py        # Script para pruebas interactivas
â”œâ”€â”€ modelo_saludos_rnn.keras         # Modelo entrenado guardado
â”œâ”€â”€ tokenizer_saludos.pickle         # Tokenizer guardado
â”œâ”€â”€ entrenamiento_metricas.png       # GrÃ¡ficas de mÃ©tricas
â””â”€â”€ README_SALUDOS.md               # Este archivo
```

---

## ğŸ“Š Dataset

**Archivo:** `saludos_dataset.txt`

- **Total de frases:** 93
- **Vocabulario:** 60 palabras Ãºnicas
- **Ejemplos:**
  - "hola"
  - "hola amigo"
  - "buenos dÃ­as"
  - "cÃ³mo estÃ¡s"
  - "quÃ© tal"

El dataset contiene saludos comunes en espaÃ±ol con diferentes variaciones y combinaciones.

---

## ğŸ”§ Proceso de Entrenamiento

### a) TokenizaciÃ³n y Padding

#### **TokenizaciÃ³n:**
Convierte palabras en nÃºmeros Ãºnicos (Ã­ndices).

```python
Ejemplo: "hola amigo" â†’ [1, 3]
```

Cada palabra del vocabulario recibe un Ã­ndice Ãºnico:
- 'hola': 1
- 'cÃ³mo': 2
- 'amigo': 3
- 'quÃ©': 4
- etc.

#### **CreaciÃ³n de Secuencias:**
Para cada frase, se crean mÃºltiples secuencias de entrenamiento:

```
Frase: "hola buenos dÃ­as"
Secuencias generadas:
  [hola] â†’ buenos
  [hola, buenos] â†’ dÃ­as
```

Esto genera 174 secuencias de entrenamiento a partir de las 93 frases originales.

#### **Padding:**
Todas las secuencias se rellenan con ceros para tener la misma longitud (longitud mÃ¡xima = 4):

```
[1, 3]        â†’ [0, 0, 1, 3]
[1, 6, 7]     â†’ [0, 1, 6, 7]
[1, 6, 7, 3]  â†’ [1, 6, 7, 3]
```

---

### b) Arquitectura del Modelo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CAPA EMBEDDING                      â”‚
â”‚ - Input dim: 60 palabras            â”‚
â”‚ - Output dim: 100 (vectores densos) â”‚
â”‚ - Convierte Ã­ndices â†’ vectores      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CAPA SimpleRNN                      â”‚
â”‚ - 150 unidades recurrentes          â”‚
â”‚ - Captura patrones temporales       â”‚
â”‚ - return_sequences=False            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CAPA DENSE (Salida)                 â”‚
â”‚ - 60 neuronas (una por palabra)     â”‚
â”‚ - ActivaciÃ³n: softmax               â”‚
â”‚ - Output: probabilidades            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ParÃ¡metros del modelo:**
- Total de parÃ¡metros: ~38,000
- Optimizador: Adam
- FunciÃ³n de pÃ©rdida: Categorical Crossentropy
- MÃ©trica: Accuracy

---

### c) Entrenamiento

**ConfiguraciÃ³n:**
- **Ã‰pocas:** 200
- **Batch size:** 32
- **ValidaciÃ³n:** 20% de los datos

**Resultados finales:**
- **PÃ©rdida (training):** 0.8738
- **PÃ©rdida (validaciÃ³n):** 8.6929
- **PrecisiÃ³n (training):** 56.83%
- **PrecisiÃ³n (validaciÃ³n):** 8.57%

**Nota:** La diferencia entre training y validaciÃ³n indica overfitting, lo cual es esperado dado el tamaÃ±o pequeÃ±o del dataset. El modelo memoriza bien los patrones de entrenamiento.

---

### d) Ejemplos de Entrada y Salida

#### Ejemplo 1: "hola"
```
Entrada: "hola"

Top 5 predicciones:
1. 'amigo'  â†’ 20.87%
2. 'seÃ±or'  â†’ 10.63%
3. 'buenas' â†’ 10.59%
4. 'amiga'  â†’ 10.45%
5. 'mundo'  â†’ 5.26%

Frase generada: "hola amigo cÃ³mo estÃ¡s hoy"
```

#### Ejemplo 2: "buenos dÃ­as"
```
Entrada: "buenos dÃ­as"

Top 5 predicciones:
1. 'a'    â†’ 28.42%
2. 'seÃ±or' â†’ 14.58%
3. 'quÃ©'   â†’ 14.49%
4. 'cÃ³mo'  â†’ 14.23%
5. 'amigo' â†’ 13.95%

Frase generada: "buenos dÃ­as a todos"
```

#### Ejemplo 3: "cÃ³mo estÃ¡s"
```
Entrada: "cÃ³mo estÃ¡s"

Top 5 predicciones:
1. 'hoy'   â†’ 33.65%
2. 'todo'  â†’ 32.90%
3. 'amigo' â†’ 32.36%
4. 'encuentras' â†’ 0.12%
5. 'va'    â†’ 0.11%

Frase generada: "cÃ³mo estÃ¡s hoy"
```

#### Ejemplo 4: "quÃ© tal"
```
Entrada: "quÃ© tal"

Top 5 predicciones:
1. 'todo'  â†’ 39.92%
2. 'estÃ¡n' â†’ 20.22%
3. 'estÃ¡s' â†’ 19.97%
4. 'amigo' â†’ 19.21%
5. 'cuentas' â†’ 0.14%

Frase generada: "quÃ© tal todo bien"
```

---

### e) EvaluaciÃ³n del DesempeÃ±o

#### **MÃ©tricas principales:**

| MÃ©trica | Training | ValidaciÃ³n |
|---------|----------|------------|
| **PÃ©rdida (Loss)** | 0.874 | 8.693 |
| **PrecisiÃ³n (Accuracy)** | 56.83% | 8.57% |

#### **AnÃ¡lisis de las grÃ¡ficas:**

![MÃ©tricas de Entrenamiento](entrenamiento_metricas.png)

**GrÃ¡fica de PÃ©rdida:**
- La pÃ©rdida de entrenamiento disminuye constantemente (de ~4.0 a ~0.87)
- La pÃ©rdida de validaciÃ³n aumenta (overfitting)
- Esto indica que el modelo memoriza bien los datos de entrenamiento

**GrÃ¡fica de PrecisiÃ³n:**
- La precisiÃ³n de entrenamiento alcanza ~60%
- La precisiÃ³n de validaciÃ³n se mantiene baja (~8%)
- El dataset es pequeÃ±o, por lo que el modelo se especializa en los ejemplos vistos

#### **InterpretaciÃ³n:**

âœ… **Aspectos positivos:**
- El modelo aprende patrones correctamente
- Las predicciones para frases conocidas son precisas
- La generaciÃ³n de texto es coherente con el dominio (saludos)

âš ï¸ **Limitaciones:**
- Overfitting debido al tamaÃ±o pequeÃ±o del dataset
- Baja generalizaciÃ³n a frases no vistas
- El modelo funciona mejor con las frases exactas del training

#### **Mejoras sugeridas:**

1. **Aumentar el dataset:** Agregar mÃ¡s variaciones de saludos
2. **RegularizaciÃ³n:** AÃ±adir Dropout layers
3. **Arquitectura:** Probar LSTM en lugar de SimpleRNN
4. **Embeddings pre-entrenados:** Usar Word2Vec o GloVe
5. **Data augmentation:** Crear mÃ¡s variaciones de las frases existentes

---

## ğŸš€ CÃ³mo Usar

### 1. Entrenar el modelo

```bash
python prediccion_saludos_RNN.py
```

Esto generarÃ¡:
- `modelo_saludos_rnn.keras` (modelo entrenado)
- `tokenizer_saludos.pickle` (tokenizer)
- `entrenamiento_metricas.png` (grÃ¡ficas)

### 2. Usar el modelo interactivamente

```bash
python inferencia_interactiva.py
```

Luego ingresa frases de prueba:
```
ğŸ“ Ingresa una frase: hola

ğŸ” TOP 5 PREDICCIONES:
   1. 'amigo' â†’ 20.87%
   2. 'seÃ±or' â†’ 10.63%
   3. 'buenas' â†’ 10.59%
   4. 'amiga' â†’ 10.45%
   5. 'mundo' â†’ 5.26%

ğŸ’¬ FRASE GENERADA:
   hola amigo cÃ³mo estÃ¡s hoy
```

---

## ğŸ“¦ Dependencias

```
tensorflow>=2.20.0
keras>=3.12.0
numpy>=2.3.0
matplotlib>=3.10.0
```

Instalar con:
```bash
pip install tensorflow keras numpy matplotlib
```

---

## ğŸ¯ Casos de Uso

1. **Autocompletado de texto** en aplicaciones de chat
2. **Sugerencias de respuesta** en sistemas de mensajerÃ­a
3. **Aprendizaje de patrones de lenguaje** en dominios especÃ­ficos
4. **GeneraciÃ³n de texto** para respuestas automÃ¡ticas

---

## ğŸ“ˆ Resultados Visuales

### Predicciones por Entrada

| Entrada | Mejor PredicciÃ³n | Confianza |
|---------|------------------|-----------|
| "hola" | "amigo" | 20.87% |
| "buenos" | "dÃ­as" | 99.89% |
| "quÃ©" | "tal" | 61.08% |
| "cÃ³mo estÃ¡s" | "hoy" | 33.65% |
| "buenas noches" | "amigo" | 50.43% |

---

## ğŸ§  Conceptos Clave Aprendidos

1. **TokenizaciÃ³n:** ConversiÃ³n de texto a nÃºmeros
2. **Padding:** NormalizaciÃ³n de longitudes de secuencia
3. **Embeddings:** RepresentaciÃ³n densa de palabras
4. **RNN:** Procesamiento de secuencias temporales
5. **Softmax:** DistribuciÃ³n de probabilidades sobre vocabulario
6. **Overfitting:** EspecializaciÃ³n excesiva en datos de entrenamiento

---

## ğŸ‘¨â€ğŸ’» Autor

Miller - Universidad CatÃ³lica Sedes Sapientiae (UCSS)
Ciclo 09 - Inteligencia Artificial

---

## ğŸ“ Notas Adicionales

- El modelo predice **una palabra a la vez**
- Las probabilidades suman 100% sobre todo el vocabulario
- El modelo puede generar secuencias completas iterativamente
- La calidad mejora significativamente con mÃ¡s datos de entrenamiento

---

## ğŸ” Experimentos Adicionales

Prueba modificar estos parÃ¡metros en `prediccion_saludos_RNN.py`:

```python
# LÃ­nea 169: Cambiar tipo de capa RNN
SimpleRNN(150) â†’ LSTM(150)  # Mejor memoria a largo plazo

# LÃ­nea 165: Cambiar dimensiÃ³n de embedding
output_dim=100 â†’ output_dim=200  # Vectores mÃ¡s ricos

# LÃ­nea 191: Aumentar Ã©pocas
epochs=200 â†’ epochs=500  # MÃ¡s entrenamiento
```

Â¡Explora y experimenta con diferentes configuraciones!
